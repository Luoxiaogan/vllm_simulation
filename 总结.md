1. 项目核心目标
这是一个在运筹学框架下，对大语言模型（LLM）服务进行流体模型ODE仿真的系统。主要目标是验证流体极限（Fluid
Limit）理论在LLM服务调度中的应用。

1. 系统架构特点

PD分离架构（Prefill-Decode Disaggregation）

- 系统假设Prefill阶段已在专用服务器完成
- 仿真专注于Decode阶段的内存管理和调度
- 请求以"准备decode"状态到达，携带prefill_length信息

四种请求状态

1. WAITING：等待调度，不占用GPU内存
2. RUNNING：正在执行，KV缓存在GPU显存中
3. SWAPPED：被抢占，KV缓存复制到CPU
4. COMPLETED：解码完成，离开系统

3. 核心仿真逻辑

批次执行模型

批次执行时间 = d_0 + d_1 * B(t)
其中B(t)为当前批次的总token数

内存管理机制

- GPU内存计算：每个RUNNING请求占用 = prefill_length + current_decode_position
- 内存约束：当GPU内存超过M_total时触发swap-out
- 批次约束：执行批次的token总数不超过B

调度优先级

1. RUNNING请求保持在批次中
2. SWAPPED请求优先恢复
3. WAITING请求最后接纳

4. 控制策略实现

默认策略（DefaultPolicy）

- 队列策略：FCFS（先来先服务）
- Victim选择：LIFO（最晚进入RUNNING的优先被swap）
- 包含预防性内存检查机制

高级策略（AdvancedPolicy）

- 支持Swap和Sacrifice两种抢占模式
- Aggressive策略：vLLM风格，严格优先级
- Conservative策略：最小化抢占

5. 仿真执行流程

1. 请求到达 → 加入WAITING队列
2. 构建批次：
    - 预防性检查内存增长
    - 恢复SWAPPED请求
    - 接纳WAITING请求
3. 执行批次：
    - 从RUNNING中选择子集（受B约束）
    - 计算执行时间
    - 推进解码位置
4. 处理完成：
    - 提取完成的请求
    - 更新系统状态
5. 循环直到所有请求完成